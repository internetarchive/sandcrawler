# OAI-PMH 2023-10 List

We ran two OAI-PMH harvests in 2023:

* [2023-06-15](https://archive.org/details/oai_harvest_20230615) (279,718,040) (a)
* [2023-11-01](https://archive.org/details/oai_harvest_2023-11-01) (326,163,077) (b)

An increase by 46,445,037. (c)

Out of the first harvest, we created a somewhat deduplicated seed list of
38,362,590 - so about 13%. Given the same ratio holds, we may find about 6M
usable URLs in the the 46M new records.

Instead of the sandcrawler based workflow, we do a CDX check on the starting
URL and keep only the URLs for which we do not have any capture yet.

* [ ] get url list from a
* [ ] get url list from b
* [ ] calculate url list c = b - a
* [ ] run CDX lookup for c
* [ ] keep non-captured list of URLs as seedlist for crawl

Used a subset of URL ("no-id", no .id TLD), for
[a](https://archive.org/download/oai_harvest_20230615/2023-06-15-metha-url-reduced-no-id.txt.zst)
(83,642,625, not 279,718,040).


```shell
$ zstdcat -T0 2023-06-15-metha-url-reduced-no-id.txt.zst | LC_ALL=C sort -T /magna/tmp -S 30% -u | zstd -c -T0 > a.txt.zst
$ zstdcat -T0 2023-11-01-metha-oai-url-uniq.ndjson.zst| LC_ALL=C sort -T /magna/tmp -S 50% -u | zstd -c -T0 > b.txt.zst
$ LC_ALL=C comm -13 <(zstdcat -T0 a.txt.zst) <(zstdcat -T0 b.txt.zst) | pv -l | grep ^http | zstd -c -T0 > c.txt.zst
$ zstdcat -T0 c.txt.zst | grep -v '[.]id/' | zstd -c -T0 > d.txt.zst
```

We end up with 180,622,741 new URLs. Let's exclude ".id" TLD for the moment: 173,388,695 w/o id.

## More filtering

Fix some obvious URL issues.

```shell
$ time zstdcat -T0 d.txt.zst | \
    sed -e 's@ @@g;s@http//@http://@g;s@https//@https://@g;s@http://http:@http://@;s@http://:@http://@;s@http.//@http://@;' | \
    grep -v "127.0.0.1" | \
    grep -v '^https?://' | \
    awk 'length($0) > 20' | \
    zstd -c -T0 > e.txt.zst

real    3m22.668s
user    7m40.435s
sys     1m24.969s
```

Still 171,689,316 URLs. Using a tiny tools
[uppsala](https://github.com/miku/oneoff/blob/main/cmd/uppsala/main.go) to
check if URL is at least parsable. Got: 171,688,861

Another filter:

```shell
$ zstdcat f.txt.zst | \
    sed -E -e 's@^http://(10[.][0-9]*)/@https://doi.org/\1/@g;s@dx.doi.org/http@doi.org@' | \
    zstd -c -T0 > g.txt.zst
```

After that, we find 11,377,589 links ending in `.pdf`. A quick CDX sampling:

```
$ zstdcat -T0 g.txt.zst | \
    grep -i '[.]pdf$' | \
    shuf -n 1000 | \
    cdxlookup -C > sample.json
```

Then:

```
$ jq -r .count < sample.json | sort | uniq -c | sort -nr
    575 0
    425 1
```

Randomly seen:

* [https://scholars.unh.edu/context/greenville_nh_reports/article/1008/viewcontent/greenville__9_.pdf](https://scholars.unh.edu/context/greenville_nh_reports/article/1008/viewcontent/greenville__9_.pdf):
  we don't have that particular URL, but IA digitized it.

So, maybe 6.5M unseen PDF, but it contains non-scholarly files (e.g.
[1](http://www.bibliotekacyfrowa.pl/Content/6583/PDF/document.pdf),
[2](https://dera.ioe.ac.uk/id/eprint/33804/1/VAT%20rule%20changes%20for%20higher%20education%20-%20GOV.pdf),
..., but also nice things, like
[1](https://digitalcommons.usu.edu/context/smallsat/article/2333/viewcontent/XI_3.pdf),
[2](https://trace.tennessee.edu/context/jaepl/article/1348/viewcontent/Ryden.pdf), ...)

## CDX lookup

Using the last cleaned version of the URLs.

```
$ zstd -T0 -dc g.txt.zst > oai-pmh-2023-12-crawl-urls.txt
$ wc -l oai-pmh-2023-12-crawl-urls.txt
171688861
```

Move to HDFS.

```
$ $ gohdfs df -h
Filesystem    Size    Used Available  Use%
           7257.8T 6651.9T    605.5T 91%
$ gohdfs put oai-pmh-2023-12-crawl-urls.txt /user/martin/
```

