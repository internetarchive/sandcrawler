# OAI-PMH crawl 06/2023

> Preparation for OAI crawl, metadata gathered with [metha](https://github.com/miku/metha)

Did a new OAI-PMH harvest via [metha](https://github.com/miku/metha) over [sites.tsv](https://raw.githubusercontent.com/miku/metha/master/contrib/sites.tsv). Harvested
content is up at:
[https://archive.org/details/oai_harvest_20230615](https://archive.org/details/oai_harvest_20230615).

The harvest was run via something like this:

```
$ while true; do timeout 600 shuf sites.tsv | parallel -u -j 200 metha-sync -T 8s -base-dir /data/.cache/metha {}; done
```

Harvests may get stuck, hence the `while true; timeout` ...

All the harvested data was concatenated into a single XML file.

```
$ fd -t file . '/data/.cache/metha/' | parallel unpigz -c | zstd -T0 -c > /data/tmp/metharaw.xml.zst
```

We then used a one-off script to convert this 130G+ compresseed, agglutinated XML and converted it to JSON:

* [xmlstream.go](https://github.com/miku/metha/blob/e38b0133559a7251e7b322dc1881e6381fa6d3a5/extra/largecrawl/xmlstream.go)

This took around 12h.

```
$ zstdcat -T0 /data/tmp/metharaw.xml.zst | ./xmlstream -D | zstd -c -T0 > /data/tmp/metharaw.json.zst
```

This result file 47G compressed (~387G uncompressed), contains 452,611,134
records, 1.36B URLs w/ dups, 279M unique URLs (some records may not have a
URL?). We also found 16M URLs matching '[.]pdf$' and a total of over 700K domains.

## Data inspection

Types coverage

    zstdcat -T0 metharaw.json.zst | pv -l | jq "select(.types != null) | .types[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > types_counts.txt
    # => 228558 types, "text", "texte", "publication en série imprimée", "printed serial", "info:eu-repo/semantics/article", "article", ...

Dump all ISSNs, with counts, quick check how many are in chocula/fatcat

    zstdcat -T0 metharaw.json.zst | pv -l | jq "select(.issn != null) | .issn[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > issn_counts.txt
    # => 29232226! as per https://github.com/miku/issnlister, there are no more than 2317440 valid issn! after running and issncheck, we get only 1381245 valid and registered ones

Language coverage

    zstdcat metharaw.json.zst | pv -l | jq "select(.languages != null) | .languages[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > languages_counts.txt
    # => 32464, "fre", "en", "eng", "EN", "English", "hrv", "ger", "spa", "deu", "und", ...

Format coverage

    zstdcat metharaw.json.zst | pv -l | jq "select(.formats != null) | .formats[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > formats_counts.txt
    => 3462109 "application/pdf", "Electronic Resource", "image/jpeg", "text", "text/html", "image/tiff", "Nombre total de vues :  1", "image/vnd.sealedmedia.softseal-jpg", ...

Have a DOI?

    zstdcat metharaw.json.zst | pv -l | rg '"doi":' | rg '"10.' | wc -l
    => 74,221,001

    zstdcat metharaw.json.zst | pv -l | jq "select(.doi != null) | .doi[]" -r | sort -u -S 5G > doi_raw.txt
    => 28,449,005

## Transform, Load, Bulk Ingest

    zstdcat -T0 metharaw.json.zst | parallel --pipe --block 10M './oai2ingestrequest.py -' | pv -l | zstd -c -T0 > oai.202306.requests.json.zst
    # =>  255M 0:16:52 [ 252k/s]

We have 255266926 ingest requests (3.4G compressed; 85G uncompressed).

### Check sandcrawler

Before we continue, make sure sandcrawler db host has enough headroom.

```
$ df -h /1
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdb1       1.7T  1.5T  231G  87% /1

$ du -hs /1/srv/postgresql/
1.2T    /1/srv/postgresql/

sandcrawler=# SELECT pg_size_pretty( pg_database_size('sandcrawler') );
 pg_size_pretty
----------------
 1167 GB
(1 row)

sandcrawler=# SELECT pg_size_pretty( pg_total_relation_size('ingest_request') );
 pg_size_pretty
----------------
 108 GB
(1 row)

   count
-----------
 213421221
(1 row)
```

We had 213,421,221 ingest requests so far. Given that we could add another 255M
(worst case), we could add 126G to the relation. That would leave about 100G of
spare disk space.

How many ingest requests did we have for OAI?

    sandcrawler=# select count(*) from ingest_request where ingest_type = 'pdf' and link_source = 'oai';

Do we need the crossref table still?

```
sandcrawler=# select pg_size_pretty(pg_total_relation_size('crossref'));
 pg_size_pretty
----------------
 471 GB
(1 row)

sandcrawler=# \d crossref
                       Table "public.crossref"
 Column  |           Type           | Collation | Nullable | Default
---------+--------------------------+-----------+----------+---------
 doi     | text                     |           | not null |
 indexed | timestamp with time zone |           | not null |
 record  | json                     |           | not null |
Indexes:
    "crossref_pkey" PRIMARY KEY, btree (doi)
Check constraints:
    "crossref_doi_check" CHECK (octet_length(doi) >= 4 AND doi = lower(doi))
```

----

## Obsoleted

## Steps

* ran `metha-sync` or `metha-snapshot` to crawl all 70K+ endpoints
* grepped out all URL like fields
* ran [urlscrub](https://github.com/miku/metha/blob/master/extra/largecrawl/urlscrub.go) url cleaner script over list

Preliminary result:

```
$ LC_ALL=C wc -l 2023-06-15-metha-url-cleaned.txt
91663562 2023-06-15-metha-url-cleaned.txt
```

Across 400K+ domains. Want to filter out domain (out of scope, too large):

```
zstdcat /srv/sandcrawler/tasks/oai-pmh/oai_pmh_partial_dump_2022_03_01.ndj.zst \
    | rg -v 'oai:kb.dk:' \
    | rg -v 'oai:bdr.oai.bsb-muenchen.de:' \
    | rg -v 'oai:hispana.mcu.es:' \
    | rg -v 'oai:bnf.fr:' \
    | rg -v 'oai:ukm.si:' \
    | rg -v 'oai:biodiversitylibrary.org:' \
    | rg -v 'oai:hsp.org:' \
    | rg -v 'oai:repec:' \
    | rg -v 'oai:n/a:' \
    | rg -v 'oai:quod.lib.umich.edu:' \
    | rg -v 'oai:americanae.aecid.es:' \
    | rg -v 'oai:www.irgrid.ac.cn:' \
    | rg -v 'oai:espace.library.uq.edu:' \
    | rg -v 'oai:edoc.mpg.de:' \
    | rg -v 'oai:bibliotecadigital.jcyl.es:' \
    | rg -v 'oai:repository.erciyes.edu.tr:' \
    | rg -v 'oai:krm.or.kr:' \
```

Or:

```
$ cat 2023-06-15-metha-url-cleaned.txt |\
     grep -v 'kb.dk/' |\
     grep -v 'bsb-muenchen.de/' |\
     grep -v 'hispana.mcu.es/' |\
     grep -v 'bnf.fr/' |\
     grep -v 'ukm.si/' |\
     grep -v 'biodiversitylibrary.org' |\
     grep -v 'hsp.org/' |\
     grep -v 'repec' |\
     grep -v 'quod.lib.umich.edu' |\
     grep -v 'americanae.aecid.es' |\
     grep -v 'irgrid.ac.cn' |\
     grep -v 'espace.library.uq.edu' |\
     grep -v 'edoc.mpg.de' |\
     grep -v 'bibliotecadigital.jcyl.es' |\
     grep -v 'erciyes.edu.tr' |\
     grep -v 'nbn-resolving.de' |\
     grep -v 'krm.or.kr' > 2023-06-15-metha-url-reduced.txt
```

Result: 88309333 urls.

Filtering out .id TLD for now: 85500802

Seemingly direct PDF links: 4477521:

```
$ grep -i 'pdf$' 2023-06-15-metha-url-reduced-no-id.txt > 2023-06-15-metha-url-reduced-pdf-only.txt
```

Direct PDF links from 59067 domains.

```
$ wc -l 2023-06-15-metha-url-reduced-pdf-only-domains.txt
59067 2023-06-15-metha-url-reduced-pdf-only-domains.txt
```

Reduce length and run urlscrub again.

```
$ wc -l 2023-06-15-metha-url-reduced-no-id.txt
83642625 2023-06-15-metha-url-reduced-no-id.txt
```

----

Fields of interest for ingest:

- oai identifer
- doi
- formats
- urls (maybe also "relations")
- types (type+stage)

