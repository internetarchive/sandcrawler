# OAI-PMH crawl 06/2023

> Preparation for OAI crawl, metadata gathered with [metha](https://github.com/miku/metha)

Did a new OAI-PMH harvest via [metha](https://github.com/miku/metha) over [sites.tsv](https://raw.githubusercontent.com/miku/metha/master/contrib/sites.tsv). Harvested
content (metadata XML, JSON) is up at:
[https://archive.org/details/oai_harvest_20230615](https://archive.org/details/oai_harvest_20230615).

The harvest was run via something like this (harvests may get stuck, hence the `while true; timeout`):

```
$ while true; do timeout 600 shuf sites.tsv | parallel -u -j 200 metha-sync -T 8s -base-dir /data/.cache/metha {}; done
```

All the harvested data was concatenated into a single XML file.

```
$ fd -t file . '/data/.cache/metha/' | parallel unpigz -c | zstd -T0 -c > /data/tmp/metharaw.xml.zst
```

We then used a one-off script to convert this 130G+ compresseed, agglutinated XML and converted it to JSON:

* [xmlstream.go](https://github.com/miku/metha/blob/e38b0133559a7251e7b322dc1881e6381fa6d3a5/extra/largecrawl/xmlstream.go)

This took around 12h.

```
$ zstdcat -T0 /data/tmp/metharaw.xml.zst | ./xmlstream -D | zstd -c -T0 > /data/tmp/metharaw.json.zst
```

This result file 47G compressed (~387G uncompressed), contains 452,611,134
records, 1.36B URLs w/ dups, 279M unique URLs (some records may not have a
URL?). We also found 16M URLs matching '[.]pdf$' and a total of over 700K domains.

## Data inspection

Types coverage

    zstdcat -T0 metharaw.json.zst | pv -l | jq "select(.types != null) | .types[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > types_counts.txt
    # => 228558 types, "text", "texte", "publication en série imprimée", "printed serial", "info:eu-repo/semantics/article", "article", ...

Dump all ISSNs, with counts, quick check how many are in chocula/fatcat

    zstdcat -T0 metharaw.json.zst | pv -l | jq "select(.issn != null) | .issn[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > issn_counts.txt
    # => 29232226! as per https://github.com/miku/issnlister, there are no more than 2317440 valid issn! after running and issncheck, we get only 1381245 valid and registered ones

Language coverage

    zstdcat metharaw.json.zst | pv -l | jq "select(.languages != null) | .languages[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > languages_counts.txt
    # => 32464, "fre", "en", "eng", "EN", "English", "hrv", "ger", "spa", "deu", "und", ...

Format coverage

    zstdcat metharaw.json.zst | pv -l | jq "select(.formats != null) | .formats[]" -r | sort -S 5G | uniq -c | sort -nr -S 1G > formats_counts.txt
    => 3462109 "application/pdf", "Electronic Resource", "image/jpeg", "text", "text/html", "image/tiff", "Nombre total de vues :  1", "image/vnd.sealedmedia.softseal-jpg", ...

Have a DOI?

    zstdcat metharaw.json.zst | pv -l | rg '"doi":' | rg '"10.' | wc -l
    => 74,221,001

    zstdcat metharaw.json.zst | pv -l | jq "select(.doi != null) | .doi[]" -r | sort -u -S 5G > doi_raw.txt
    => 28,449,005

Of these DOI, how many have we seen already?

* used the recent `release_extid.tsv.gz` from [https://archive.org/details/fatcat_bulk_exports_2023-06-09](https://archive.org/details/fatcat_bulk_exports_2023-06-09) to extract DOI
* normalized to lowercase
* DOI list from OAI metadata (lowercase)

```
$ LC_ALL=C comm -23 oai-doi_sorted.txt fatcat-doi_sorted.txt > oai-doi-only.txt
$ wc -l oai-doi-only.txt
5463326 oai-doi-only.txt
```

Running a quick DOI check, via [doicache]():

```
$ shuf -n 200 oai-doi-only.txt | ./doicache
H404    10.1016/j.watres.2011.09.024&#x27e9;    NOTAVAILABLE
H404    10.5194/acp‐8‐3671‐2008 NOTAVAILABLE
OK  10.1080/00015385.2023.2182985   https://www.tandfonline.com/doi/full/10.1080/00015385.2023.2182985
H404    10.1086/283561. NOTAVAILABLE
OK  10.1371/journal.pone.0004342.g002   https://dx.plos.org/10.1371/journal.pone.0004342.g002
H404    10.24252/join.v2i2.3978 NOTAVAILABLE
OK  10.1371/journal.pone.0117697.g003   https://dx.plos.org/10.1371/journal.pone.0117697.g003
OK  10.1007/978-3-662-65584-9   https://link.springer.com/10.1007/978-3-662-65584-9
OK  10.33508/peka.v1i2.2800 http://journal.wima.ac.id/index.php/peka/article/view/2800
...
```

* 112 OK, 88 H404, 3 ""
* so 56% seem to be valid, so still, 3059462 valid DOI, that we do not have in the catalog


## Transform, Load, Bulk Ingest

    zstdcat -T0 metharaw.json.zst | parallel --pipe --block 10M './oai2ingestrequest.py -' | pv -l | zstd -c -T0 > oai.202306.requests.json.zst
    # =>  255M 0:16:52 [ 252k/s]

We have 255,266,926 ingest requests (3.4G compressed; 85G uncompressed).

### Check sandcrawler

Before we continue, make sure sandcrawler db host has enough headroom.

```
$ df -h /1
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdb1       1.7T  1.5T  231G  87% /1

$ du -hs /1/srv/postgresql/
1.2T    /1/srv/postgresql/

sandcrawler=# SELECT pg_size_pretty( pg_database_size('sandcrawler') );
 pg_size_pretty
----------------
 1167 GB
(1 row)

sandcrawler=# SELECT pg_size_pretty( pg_total_relation_size('ingest_request') );
 pg_size_pretty
----------------
 108 GB
(1 row)

   count
-----------
 213421221
(1 row)
```

We had 213,421,221 ingest requests so far. Given that we could add another 255M
(worst case), we could add 126G to the relation. That would leave about 100G of
spare disk space.

How many ingest requests did we have for OAI?

    sandcrawler=# select count(*) from ingest_request where ingest_type = 'pdf' and link_source = 'oai';
    #   count
    # ----------
    #  58214767
    # (1 row)


Do we need the crossref table still?

```
sandcrawler=# select pg_size_pretty(pg_total_relation_size('crossref'));
 pg_size_pretty
----------------
 471 GB
(1 row)

sandcrawler=# \d crossref
                       Table "public.crossref"
 Column  |           Type           | Collation | Nullable | Default
---------+--------------------------+-----------+----------+---------
 doi     | text                     |           | not null |
 indexed | timestamp with time zone |           | not null |
 record  | json                     |           | not null |
Indexes:
    "crossref_pkey" PRIMARY KEY, btree (doi)
Check constraints:
    "crossref_doi_check" CHECK (octet_length(doi) >= 4 AND doi = lower(doi))
```

An option is to `pg_dump` the `crossref` table. Another option is to trim down
the ingest request list, before moving forward.

First filter (filter-1) run:

```shell
martin@martin-dev:/tmp/scratch$ zstdcat -T0 oai.202306.requests.json.zst | \
     grep -v 'oai:kb.de:' | \
     grep -v 'bdr.oai.bsb-muenchen.de:' | \
     grep -v 'oai:hispana.mcu.es:' | \
     grep -v 'oai.bnf:' | \
     grep -v 'oai:ukm.si:' | \
     grep -v 'oai:biodiversity.org:' | \
     grep -v 'oai:hsp.org:' | \
     grep -v 'oai:repec:' | \
     grep -v 'oai:n/a:' | \
     grep -v 'oai:quod.lib.umich.edu:' | \
     grep -v 'oai:americanae.aecid.es:' | \
     grep -v 'oai:www.irgird.ac.cn:' | \
     grep -v 'oai:www.irgrid.ac.cn:' | \
     grep -v 'oai:espace.library.uq.edu:' | \
     grep -v 'oai:edoc.mpg.de:' | \
     grep -v 'oai_bibliotecadigital.jcyl.es:' | \
     grep -v 'oai:bibliotecadigital.jcyl.es:' | \
     grep -v 'oai:repository.erciyes.edu.tr:' | \
     grep -v 'oai:krm.or.kr:' | \
     pv -l | \
     zstd -c -T0 > oai.202306.requests.filter-1.json.zst
```

Went from 255,266,926 ingest request to: ...

Next steps:

* persist the ingest requests in the DB, via `persist_tool.py`

The `persist_tool.py` take JSON and will insert rows into `ingest_request`

```sql
    INSERT INTO
    ingest_request (link_source, link_source_id, ingest_type, base_url, ingest_request_source, release_stage, request)
    VALUES %s
    ON CONFLICT ON CONSTRAINT ingest_request_pkey DO
```

On conflict we do `NOTHING`.

```json
{
  "base_url": "http://103.85.61.66/ojs/index.php/jwk/article/view/170",
  "edit_extra": {},
  "ext_ids": {
    "doi": "10.31845/jwk.v23i1.170",
    "oai": "oai:ojs.jwk.bandung.lan.go.id:article/170"
  },
  "ingest_request_source": "metha-bulk",
  "ingest_type": "pdf",
  "link_source": "oai",
  "link_source_id": "oai:ojs.jwk.bandung.lan.go.id:article/170",
  "rel": null,
  "release_stage": "published"
}
```

The `request` field contains extra/optional fields, like "ext_ids",
"edit_extra", "rel" and others. These are considered at "insert_ingest_request"
(at db insert time), but also before in "request_to_row".

The `ingest_request` PKEY is `(link_source, link_source_id, ingest_type,
base_url)` - so we duplicates will not be inserted.

```
sandcrawler=# \d ingest_request
                           Table "public.ingest_request"
        Column         |           Type           | Collation | Nullable | Default
-----------------------+--------------------------+-----------+----------+---------
 link_source           | text                     |           | not null |
 link_source_id        | text                     |           | not null |
 ingest_type           | text                     |           | not null |
 base_url              | text                     |           | not null |
 ingest_request_source | text                     |           |          |
 created               | timestamp with time zone |           | not null | now()
 release_stage         | text                     |           |          |
 request               | jsonb                    |           |          |
Indexes:
    "ingest_request_pkey" PRIMARY KEY, btree (link_source, link_source_id, ingest_type, base_url)
    "ingest_request_base_url_idx" btree (base_url, ingest_type)
    "ingest_request_source_created_idx" btree (ingest_request_source, created)
Check constraints:
    "ingest_request_base_url_check" CHECK (octet_length(base_url) >= 1)
    "ingest_request_ingest_request_source_check" CHECK (octet_length(ingest_request_source) >= 1)
    "ingest_request_ingest_type_check" CHECK (octet_length(ingest_type) >= 1)
    "ingest_request_link_source_check" CHECK (octet_length(link_source) >= 1)
    "ingest_request_link_source_id_check" CHECK (octet_length(link_source_id) >= 1)
    "ingest_request_release_stage_check" CHECK (octet_length(release_stage) >= 1)

```

We can distinguish newer ingest request by the `created` column.

Some more scouting. So far, 199M ingest file results. For OAI:

```
sandcrawler=# select count(*) from ingest_file_result;
   count
-----------
 199185430
(1 row)
```

About 20M successful captures.

```
sandcrawler=# select ingest_file_result.status, COUNT(*) FROM ingest_request
    LEFT JOIN ingest_file_result
    ON ingest_file_result.ingest_type = ingest_request.ingest_type
    AND ingest_file_result.base_url = ingest_request.base_url
    WHERE ingest_request.ingest_type = 'pdf' AND ingest_request.link_source = 'oai'
    GROUP BY status
    ORDER BY COUNT DESC LIMIT 20;

       status        |  count
---------------------+----------
 success             | 20749080
 no-capture          | 15930221
 no-pdf-link         | 15789081
 redirect-loop       |  2745205
 terminal-bad-status |  1276722
 wrong-mimetype      |   715735
 link-loop           |   699800
 null-body           |    98622
 skip-wall           |    57808
 cdx-error           |    33328
 empty-blob          |    23732
 petabox-error       |    23333
 blocked-cookie      |    16653
                     |    15425
 wayback-error       |    12264
 skip-url-blocklist  |    10866
 max-hops-exceeded   |     5505
 blocked-wall        |     3060
 bad-redirect        |     2977
 body-too-large      |     2741
(20 rows)
```

Do a manual match first: extract `link_source, link_source_id, ingest_type,
base_url` from ingest request file and export that data from the table.

```
$ zstdcat -T0 oai.202306.requests.json.zst | \
    jq -rc '[.link_source, .link_source_id, .ingest_type, .base_url] | @tsv' | \
    pv -l | zstd -c -T0 > oai.202306.requests.pkey.tsv.zst
```

Interestingly, when we only look at the pkey fields, it boils down to 85M
unique lines: 85,233,193.

Now, compare those 85M ingest requests to existing ingest requests.

Piped postgres output to `dev` machine to save space.

```
$ time ssh wbgrp-svc506 "sudo -u postgres psql -d sandcrawler -c \"COPY (select link_source, link_source_id, ingest_type, base_url FROM ingest_request limit 10) TO STDOUT CSV DELIMITER E'\t';\""
```

> Just thinking: it would be nice to have a single data query and acquistion
tool; to save typing escaped SQL, elasticsearch, etc.

After comparing existing ingest requests, we have 60972261, ingest requests,
which we have not seen before. Of these, we have 6921607 ending in `[.]pdf$`, a random sample:

```
http://www.fdp-bw.de/docs/Regierungsprogramm2006_2011.pdf
https://scholarworks.umt.edu/context/mansfield_speeches/article/1006/viewcontent/Mss65_S21_B36_F15.pdf
https://normandie-univ.hal.science/hal-02454696/file/PolletM_JEurCeramSoc_2003.pdf
https://digitalcommons.wustl.edu/context/open_access_pubs/article/10498/viewcontent/AbnormallyIncreasedCarotid.pdf
https://pureadmin.qub.ac.uk/ws/files/238596263/jcm_10_01553.pdf
http://eprints.undip.ac.id/78256/1/Response_of_cocoa_trees_(Theobroma_cacao)_to_a_13-month_desiccation_period_in_Sulawesi%2C_Indonesia_(1).pdf
https://www.spc.int/DigitalLibrary/Doc/SPC/Bulletins/Circulaires_Information/13930_1970_Circulaire_dinformation__26.pdf
https://mpra.ub.uni-muenchen.de/105369/1/MPRA_paper_105369.pdf
https://pure.rug.nl/ws/files/14542324/10_samenvat.pdf
https://scholarworks.umt.edu/context/newsreleases/article/18149/viewcontent/um_pr_2001_03_16a.pdf
http://bibliotekacyfrowa.eu/Content/37526/42890.pdf
https://tigerprints.clemson.edu/context/all_theses/article/1669/viewcontent/SUN_clemson_0050M_10401.pdf
https://laender-analysen.de/api-v2/ukraine-analysen/185/UkraineAnalysen185-staatsdumawahlen_auf_der_krim-szakonyi-2017.pdf
http://studentsrepo.um.edu.my/6232/3/tengku_ubaidillah.pdf
https://hal.science/hal-03873552/file/2015_Kohlrausch2015-Lakens-Osses-pres-Archie.pdf
```

Out of a sample of 100 URLs ending in PDF, CDX lookup yields 27 results, so
best case we would have 6921607 * 0.74; about 5M new PDF.

Counting domains across the 60972261 URLs. We find 178536 domains.

```
$ time zstdcat -T0 oai.202306.only.tsv.zst | cut -f4 | awk -F / '{print $3}' | LC_ALL=C sort -S30% | uniq -c | sort -nr -S 20% > oai.202306.domain.txt
```

Top 40 domains (`*` to be excluded):

```
5177286 prensahistorica.mcu.es                        *
2507530 bvbr.bib-bvb.de:8991                          *
1954803 hal.science
1079728 www.persee.fr
 797241 dialnet.unirioja.es
 767529 www.redalyc.org
 720767 publications.rwth-aachen.de
 662935 lawcat.berkeley.edu
 630568 curis.ku.dk
 623562 acikbilim.yok.gov.tr
 514185 nbn-resolving.org
 493768 juser.fz-juelich.de
 464886 research.vu.nl
 452848 sbc.org.pl
 449859 research-information.bris.ac.uk
 436537 www.scopus.com
 415029 openrepository.ru
 411169 research.tue.nl
 406671 bvpb.mcu.es
 364977 bib-pubdb1.desy.de
 352642 escholarship.org
 345987 shs.hal.science
 342610 research.rug.nl
 340657 jbc.bj.uj.edu.pl
 338329 urn.nsk.hr
 324058 theses.hal.science
 310192 sitereports.nabunken.go.jp
 296245 cris.maastrichtuniversity.nl
 287902 hal.inrae.fr
 285934 ntur.lib.ntu.edu.tw
 268171 repository.gsi.de
 255408 www.juntadeandalucia.es
 251526 apps.who.int
 244437 research.manchester.ac.uk
 242828 repozitorij.unizg.hr
 239854 urn.kb.se
 239377 research.wur.nl
 233675 openresearch-repository.anu.edu.au
 222419 pure.rug.nl
 220363 server15795.contentdm.oclc.org
```

So exclude the top two sites:

```
5177286 prensahistorica.mcu.es                        *
2507530 bvbr.bib-bvb.de:8991                          *
```

When excluding the top two sites, how many PDF links do we find? Still, 6921579
direct PDF links.


## Bulk ingest

First, load ingest request into sandcrawler db. Use the deduplicated list with
the top-2 domains excluded; sorted, unique.

```
$ cat exclude-domains.txt
prensahistorica.mcu.es
bvbr.bib-bvb.de

$ zstdcat -T0 oai.202306.requests.json.zst |
    LC_ALL=C rg -v -f exclude-domains.txt |
    zstd -c -T0 |
    LC_ALL=C sort -u -S50% > oai.202306.reduced.json.zst
```

We are down to 78M ingest requests.

```
$ zstdcat -T0 oai.202306.request_reduced.json.zst | wc -l
78901722
```

Running last minute cleanup of urls.

```python
#!/usr/bin/env python

# adhoc last-minute fixes based on manual inspection of metadata

import fileinput
import json
import urllib.parse

min_url_length = len("http://x.id/a")

for line in fileinput.input():
    doc = json.loads(line)
    url = doc["base_url"]
    url = url.replace(" ", "")
    url = url.replace("http%EF%BC%9A", "http://")
    url = url.replace("http.//10.1002/", "http://doi.org/10.1002/")
    url = url.replace("http.//10.1007/", "http://doi.org/10.1007/")
    url = url.replace("http.//10.1016/", "http://doi.org/10.1016/")
    url = url.replace("http.//10.1080/", "http://doi.org/10.1080/")
    url = url.replace("http.//10.1107/", "http://doi.org/10.1107/")
    url = url.replace("http//", "http://")
    url = url.replace("http:///", "http://")
    url = url.replace("http://:", "http://")
    url = url.replace("https//", "https://")
    url = url.replace("https://:", "https://")
    if url.count('%') > 3:
        url = urllib.parse.unquote(url)
    if url == "http://doi.org/":
        continue
    if len(url) < min_url_length:
        continue
    if "://" not in url:
        continue
    doc["base_url"] = url
    print(json.dumps(doc))
```

The cleaned file contains 78900439 ingest requests.

```
$ zstdcat /srv/sandcrawler/tasks/oai-2023-06-13/oai.2023-06-13.request_reduced_cleaned.json.zst | pv -l | ./persist_tool.py ingest-request -
```

This will run `PersistIngestRequestWorker`, which only inserts row and reports
inserts and updates. About 5k/s.

Estimating about 20% new, or 15M.

At start, we have 221GB spare disk space on `/1`.

Got to about 77.2M, before hitting some `NUL` in the data.

```
(python) sandcrawler@wbgrp-svc506:/srv/sandcrawler/src/python$ time zstdcat -T0 /srv/sandcrawler/tasks/oai-2023-06-13/oai.2023-06-13.request_reduced_cleaned.json.zst | pv -l | ./persist_tool.py ingest-request -
^[1.68M 0:05:44 [7.52k/s] [                                                                                            <=>            Traceback (most recent call last):     <=>                                                                                           ]
  File "./persist_tool.py", line 311, in <module>
    main()
  File "./persist_tool.py", line 307, in main
    args.func(args)
  File "./persist_tool.py", line 119, in run_ingest_request
    pusher.run()
  File "/1/srv/sandcrawler/src/python/sandcrawler/workers.py", line 397, in run
    self.worker.push_batch(batch)
  File "/1/srv/sandcrawler/src/python/sandcrawler/persist.py", line 346, in push_batch
    resp = self.db.insert_ingest_request(self.cur, irequests)
  File "/1/srv/sandcrawler/src/python/sandcrawler/db.py", line 459, in insert_ingest_request
    resp = psycopg2.extras.execute_values(cur, sql, rows, page_size=250, fetch=True)
  File "/1/srv/sandcrawler/src/python/.venv/lib/python3.8/site-packages/psycopg2/extras.py", line 1296, in execute_values
    parts.append(cur.mogrify(template, args))
ValueError: A string literal cannot contain NUL (0x00) characters.
77.2M 3:41:34 [5.81k/s] [            <=>                                                                                             ]
zstd: error 70 : Write error : cannot write decoded block : Broken pipe
```

Cannot find `NUL` in input file, neither with Python or sed. What was the last
inserted ingest request from OAI?

Shortly after start, we had 215M ingest requests; after run, we have 267M, so
added 52M, and we are down to 189G of space space.

Debugging the failed file. The last `ingest_request` id id was:

```
oai:www.wellbeingintlstudiesrepository.org:pascani-1002
```

in line 77236400. Trying to reproduce the error. First, we skip 77236400 - 200 (batchsize) lines:

```
$ time zstdcat -T0 oai.2023-06-13.request_reduced_cleaned.json.zst | sed -n "$((77236200+1))"',$p' | zstd -c -T0 > oai.2023-06-13.request_reduced_cleaned_rest.json.zst
```

Trying to re-add 1.6M ingest requests:

```
$ zstdcat -T0 oai.2023-06-13.request_reduced_cleaned_rest.json.zst|wc -l
1664239

$ time zstdcat -T0 /srv/sandcrawler/tasks/oai-2023-06-13/oai.2023-06-13.request_reduced_cleaned_rest.json.zst | pv -l | ./persist_tool.py ingest-request -
```

Only after skipping 400 lines we can proceed.

```
time zstdcat -T0 /srv/sandcrawler/tasks/oai-2023-06-13/oai.2023-06-13.request_reduced_cleaned_rest.json.zst | sed -n "$((400))"',$p' | pv -l | ./persist_tool.py ingest-request -
Worker: Counter({'total': 1663840, 'insert-requests': 1168868, 'update-requests': 0})
JSON lines pushed: Counter({'total': 1663840, 'pushed': 1663840})
```

Found the offending line:

```
{"base_url":
"https://www.researchgate.net/profile/Joost-Buecker/publication/300816621_Cultural_Intelligence_as_a_Key_Construct_for_Global_Talent_Management/links/573f0b1308aea45ee844f216/Cultural-Intelligence-as-a-Key-Construct-for-Global-Talent-Management.pdf#\ufffd\ufffd\u0000C\u0000h\u0000a\u0000p\u0000t\u0000e\u0000r\u0000b\u00009\u00007\u00008\u0000-\u00003\u0000-\u00003\u00001\u00009\u0000-\u00000\u00005\u00001\u00002\u00005\u0000-\u00003\u0000_\u00007",
"edit_extra": {}, "ext_ids": {"oai":
"oai:pure.atira.dk:publications/1dcf3b81-5f2b-4005-9e44-0ee6c7d3b502"},
"ingest_request_source": "metha-bulk", "ingest_type": "pdf", "link_source":
"oai", "link_source_id":
"oai:pure.atira.dk:publications/1dcf3b81-5f2b-4005-9e44-0ee6c7d3b502", "rel":
null, "release_stage": null}
```

Finally, all except the one offending line imported:

```
$ grep -v '\u0000' /srv/sandcrawler/tasks/oai-2023-06-13/broken.ndjson | ./persist_tool.py ingest-request -
Worker: Counter({'total': 399, 'insert-requests': 12, 'update-requests': 0})
JSON lines pushed: Counter({'total': 399, 'pushed': 399})
```

We have 53M ingest request.

```
sandcrawler=# select count(*) from ingest_request where ingest_request_source = 'metha-bulk' and created > '2023-07-27'::date;
  count
----------
 53521467
(1 row)

```

> How many of these are URLs we have seen and ingested already?

```sql
select ingest_file_result.status, COUNT(*)
FROM ingest_request
LEFT JOIN ingest_file_result
ON ingest_file_result.ingest_type = ingest_request.ingest_type
AND ingest_file_result.base_url = ingest_request.base_url
WHERE
ingest_request.ingest_type = 'pdf'
AND ingest_request.link_source = 'oai'
and ingest_request.created > '2023-07-27'::date
GROUP BY status
ORDER BY COUNT DESC
LIMIT 20;


         status          |  count
-------------------------+----------
                         | 50344020
 success                 |  2203069
 no-pdf-link             |   414517
 link-loop               |   182359
 no-capture              |   166568
 redirect-loop           |    85044
 terminal-bad-status     |    79840
 wrong-mimetype          |    32668
 null-body               |     5397
 empty-blob              |     1957
 cdx-error               |     1815
 blocked-cookie          |     1452
 petabox-error           |      939
 skip-url-blocklist      |      572
 wayback-error           |      472
 wayback-content-error   |      314
 invalid-host-resolution |      111
 body-too-large          |       92
 blocked-wall            |       55
 forbidden               |       51
(20 rows)

```

Dump ingest requests, where status is NULL:

```
    COPY (
        SELECT row_to_json(ingest_request.*)
        FROM ingest_request
        LEFT JOIN ingest_file_result
            ON ingest_file_result.ingest_type = ingest_request.ingest_type
            AND ingest_file_result.base_url = ingest_request.base_url
        WHERE
            ingest_request.ingest_type = 'pdf'
            AND ingest_request.link_source = 'oai'
            AND date(ingest_request.created) > '2023-07-27'
            AND ingest_file_result.status IS NULL
    ) TO '/srv/sandcrawler/tasks/oai-2023-06-13/oai_noingest_20230613.rows.json';
    # => COPY 50344020
```

About 18G file; compressed (-9): 1.5G.

Transform to ingest requests:

```
$ zstdcat -T0 /srv/sandcrawler/tasks/oai-2023-06-13/oai_noingest_20230613.rows.json.zst | rg -v "\\\\" | ./scripts/ingestrequest_row2json.py |  | pv -l | shuf | zstd -c -T0 > /srv/sandcrawler/tasks/oai-2023-06-13/oai_noingest_20230613.ingest_request.json.zst
```

Many lines with invalid JSON;

## Feed into "bulk" topic

Run a test first; brokers are 280, 281, 285; [http://wbgrp-svc351.us.archive.org:9099/clusters/prod/topics/sandcrawler-prod.ingest-file-requests-bulk](http://wbgrp-svc351.us.archive.org:9099/clusters/prod/topics/sandcrawler-prod.ingest-file-requests-bulk)

285 has 1.3T spare space, using that.

```
$ zstdcat -T0 oai_noingest_20230613.ingest_request.json.zst | head -10 | jq . -c | kafkacat -P -b wbgrp-svc285.us.archive.org -t sandcrawler-prod.ingest-file-requests-bulk -p -1
real    21m3.205s
user    21m18.456s
sys     3m8.983s
```

Feeding the 50329035 ingest requests into the bulk queue, will start 8 workers on 506.

```
$ sudo systemctl start sandcrawler-ingest-file-bulk-worker@{1..8}.service
```

Monitoring progress:

```
$ journalctl -xfu sandcrawler-ingest-file-bulk-worker@3.service | grep no-capture
```

Seeing about 20msg/s in kafka admin. Would need a month to get through all messages. May start more workers.

Enqueued all ingest_requests into the bulk topic after 21min.

Glimpse into "ingest_file_result" from 2023:

```
sandcrawler=# select status, count(*) from ingest_file_result where updated > '2022-12-31' group by status order by count(*) desc;
               status               |  count
------------------------------------+---------
 spn2-backoff                       | 2933183
 no-pdf-link                        | 2514956
 success                            |  678251
 spn2-error:job-failed              |  207961
 terminal-bad-status                |  198726
 wrong-mimetype                     |  110766
 spn2-error                         |   73473
 link-loop                          |   65999
 skip-url-blocklist                 |   45662
 spn2-cdx-lookup-failure            |   33027
 cdx-error                          |   31873
 bad-redirect                       |   22490
 WorkerLostError                    |   20831
 no-capture                         |   20292
 wayback-error                      |   11932
 redirect-loop                      |   10673
 blocked-cookie                     |    6049
 spn2-wayback-error                 |    4351
 blocked-wall                       |    3620
 body-too-large                     |    2660
 spn2-error:browser-running-error   |    2455
 gateway-timeout                    |    1863
 wayback-content-error              |     940
 spn2-recent-capture                |     742
 timeout                            |     733
 empty-blob                         |     639
 <class 'Exception                  |     628
 petabox-error                      |     555
 spn2-error:invalid-url-syntax      |     454
 BrowserCrash                       |     353
 invalid-host-resolution            |     304
 unknown-scope                      |     255
 TimeoutError                       |      59
 SpnException                       |      55
 GatewayTimeout                     |      34
 bad-gzip-encoding                  |      33
 ConnectionError                    |      28
 wrong-scope                        |      23
 spn2-error:blocked-url             |      23
 html-resource-no-capture           |       8
 spn2-error:filesize-limit          |       6
 spn2-error:celery                  |       4
 ProxyError                         |       2
 <class 'TypeError                  |       2
 spn2-error:too-many-daily-captures |       1
 BrowsingTimeout                    |       1
 redirects-exceeded                 |       1
 <class 'ValueError                 |       1
 spn2-error:proxy-error             |       1
(49 rows)
```

Most recent stats:

```
sandcrawler=# select status, count(*) from ingest_file_result where updated > '2023-07-30' group by status order by count(*) desc;
              status              | count
----------------------------------+--------
 spn2-backoff                     | 432644
 no-pdf-link                      |  65607
 no-capture                       |  26757
 success                          |  12298
 spn2-cdx-lookup-failure          |   4426
 spn2-error                       |   3807
 terminal-bad-status              |   3458
 spn2-error:job-failed            |   2165
 gateway-timeout                  |    829
 wrong-mimetype                   |    665
 skip-url-blocklist               |    508
 redirect-loop                    |    435
 cdx-error                        |    321
 bad-redirect                     |    246
 link-loop                        |    197
 petabox-error                    |    116
 wayback-error                    |    101
 spn2-error:browser-running-error |     85
 blocked-wall                     |     64
 blocked-cookie                   |     35
 empty-blob                       |     23
 <class 'Exception                |     17
 spn2-wayback-error               |     10
 GatewayTimeout                   |      9
 wayback-content-error            |      9
 body-too-large                   |      8
 invalid-host-resolution          |      2
 spn2-error:invalid-url-syntax    |      1
 timeout                          |      1
 spn2-recent-capture              |      1
 TimeoutError                     |      1
 redirects-exceeded               |      1
(32 rows)
```
